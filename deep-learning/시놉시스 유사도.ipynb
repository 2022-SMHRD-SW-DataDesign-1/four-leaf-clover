{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8b8a163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ab733b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f172dbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ko-sentence-transformers\n",
      "  Downloading ko_sentence_transformers-0.3.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [6 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 36, in <module>\n",
      "    File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "    File \"C:\\Users\\smhrd\\AppData\\Local\\Temp\\pip-install-8zj148vf\\ko-sentence-transformers_28e6ce45e4a74361a06b887bba2cd3d7\\setup.py\", line 6, in <module>\n",
      "      long_description = f.read()\n",
      "  UnicodeDecodeError: 'cp949' codec can't decode byte 0xec in position 30: illegal multibyte sequence\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "pip install ko-sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f56774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 86.0/86.0 kB 4.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 11.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from sentence_transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from sentence_transformers) (0.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from sentence_transformers) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from sentence_transformers) (1.7.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 10.6 MB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "     ------------------------------------- 190.3/190.3 kB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (22.0)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.11.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\smhrd\\appdata\\roaming\\python\\python37\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.9.0)\n",
      "Requirement already satisfied: requests in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.5)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp37-cp37m-win_amd64.whl (268 kB)\n",
      "     ------------------------------------- 268.0/268.0 kB 16.1 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 11.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from nltk->sentence_transformers) (1.1.1)\n",
      "Requirement already satisfied: click in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from nltk->sentence_transformers) (8.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from torchvision->sentence_transformers) (9.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\smhrd\\anaconda3\\envs\\opencv3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.13)\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py): started\n",
      "  Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=b8f21324dc9716ab7a18d4edb349d800da03a2bd59434c16ec1c770f2df16ad1\n",
      "  Stored in directory: c:\\users\\smhrd\\appdata\\local\\pip\\cache\\wheels\\83\\71\\2b\\40d17d21937fed496fb99145227eca8f20b4891240ff60c86f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: tokenizers, sentencepiece, regex, huggingface-hub, transformers, nltk, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.12.0 nltk-3.8.1 regex-2022.10.31 sentence_transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.13.2 transformers-4.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66fa13",
   "metadata": {},
   "source": [
    "### Sbert 사용 하는데 순서\n",
    "\n",
    "3. 명사 단위 토큰화해서 한번 ㄱ\n",
    "\n",
    "후보 모델\n",
    "\n",
    "paraphrase-multilingual-mpnet-base-v2\n",
    "무난\n",
    "\n",
    "paraphrase-multilingual-MiniLM-L12-v2\n",
    "==> 임베딩 단어 vector값 너무 멀어\n",
    "\n",
    "distiluse-base-multilingual-cased-v1\n",
    "===> 단어 단위 일치도가 유사도 벡터값 의미 x?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bd8a927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../웹툰최종데이터.csv', encoding='euc-kr')\n",
    "data2 = pd.read_csv('../시놉시스요약ansi.csv', encoding='cp949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "df58d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paraphrase-multilingual-mpnet-base-v2\n",
    "# jhgan/ko-sbert-nli \n",
    "# jhgan/ko-sbert-sts\n",
    "# paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9767e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"jhgan/ko-sbert-sts\")\n",
    "embeddings = model.encode(data['synopsis'], convert_to_tensor=True)\n",
    "embeddings2 = model.encode(test, convert_to_tensor=True)\n",
    "# embeddings2 = model.encode(data['synopsis'],\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "cosine_scores = cosine_scores.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "948872b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "입력 시놉시스 : '여자도 군대에 간다면?'본격 여자도 군대 가는 만화!\n",
      "\n",
      " 젤 비슷한 시놉시스:\n",
      "결혼까지 망상했어! ::  10대 안에 한 번이라도 연애를 해보는 게 소원이지만 호감 상대와 결혼까지 망상하면 절대 성사되지 않는다는 징크스를 지닌 주인공. 연애의 '연'자도 허용되지 않던 그녀의 현실에도 볕 들 날이 오긴 하는데... 이게 과연 썸이 맞긴 할까? 망상 소녀 소망의 유쾌 발랄 로맨스. tensor(0.3268)\n",
      "\n",
      "\n",
      "여자를 사귀고 싶다 ::  23년간 한 번도 연애를 해보지 못한 모태솔로 주인공. 군복무를 마치고 이제 막 대학교에 복학한 그의 꿈은 단 하나. 여자친구를 사귀어 보는 것. 여자에 대해 모든 것이 낯선 그에게 자칭 연애고수 친구들의 전문적인 코칭이 시작된다. 민수는 과연 모태솔로를 탈출하고 운명같은 사랑을 쟁취할 수 있을까? tensor(0.3264)\n",
      "\n",
      "\n",
      "연애의 기록 ::  사회 초년생인 그에게는 직업이나 연봉보다 더 중요한 것이 있었으니. 그건 바로 모태솔로 탈출. 그는 꿈에 그리던 첫 연애를 시작하게 되지만 예상치 못한 고난들을 겪게 된다.  소년과 군인을 거쳐 사회인으로 거듭난 그의 사랑과 일 그리고 꿈을 다룬 성장 이야기! tensor(0.3191)\n",
      "\n",
      "\n",
      "대위님! 이번 전쟁터는 이곳인가요? ::  대한민국의 내로라 하는 엘리트 특전사 대위. 뼛속까지 군인인 그녀의 인생에 로맨스 따위는 없었다. 그러나 해외파병 중 포탄을 맞고 정신을 잃은 그녀 앞에 전혀 다른 세상이 펼쳐진다. 친구가 쓴 로맨스 판타지 소설에 빙의된 것. 그것도 비루한 삶을 살아가는 엑스트라가 되어. 깊은 한숨을 내쉬며 머리를 짚는 것도 잠깐, 그녀는 이곳을 전쟁터라 여기고 삶을 바꾸기로 한다. 절대적 카리스마로 공작가를 정복해나가는 여군. 하지만 의도치 않게 로맨스까지 휘어잡게 되는데. 원작 남주인공은 여주인공이 아닌 그녀에게 무릎까지 꿇는다. 사랑은 안중에도 없었던 대위님, 이번 전쟁터에서도 승리할 수 있을까? tensor(0.3147)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 샘플코드\n",
    "top_k = 5\n",
    "jj=1\n",
    "#We use np.argpartition, to only partially sort the top_k results\n",
    "top_results = np.argpartition(-cosine_scores[jj], range(top_k))[0:top_k]\n",
    "# 제일 높은 얘의 index 구하는거임. \n",
    "\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"입력 시놉시스 :\", data['synopsis'][jj])\n",
    "print(\"\\n 젤 비슷한 시놉시스:\")\n",
    "\n",
    "k = 0\n",
    "for idx in top_results[0:top_k]:\n",
    "    if k ==0 :\n",
    "        k+=1\n",
    "        continue\n",
    "    print(data['title'][int(idx)]+\" :: \",data['synopsis'][int(idx)].strip(),  cosine_scores[jj][int(idx)])\n",
    "    print(\"\\n\")\n",
    "    test_list = cosine_scores[jj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "70f99d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0154)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "66cb3fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_list = []\n",
    "title_list =[]\n",
    "temp_list = []\n",
    "temp_list2 = []\n",
    "top10 = 10\n",
    "\n",
    "for i in range(len(data['synopsis'])):\n",
    "    #We use np.argpartition, to only partially sort the op results\n",
    "    top_results = np.argpartition(-cosine_scores[i], range(top10))[0:top10]\n",
    "    # 제일 높은 얘의 index 구하는거임. \n",
    "\n",
    "    \n",
    "    k = 0\n",
    "    temp_list=[]\n",
    "    temp_list2=[]\n",
    "    for idx in top_results[0:top10]:\n",
    "        temp_list.append(data['title'][int(idx)])\n",
    "        temp_list2.append(cosine_scores[i][int(idx)])\n",
    "    title_list.append(temp_list)\n",
    "    similarity_list.append(temp_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ee9aa52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output =pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6bac6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "output['title'] =data['title']\n",
    "output['simtitles']=title_list\n",
    "output['cosim']=similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "89b46fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 561 entries, 0 to 560\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      561 non-null    object\n",
      " 1   simtitles  561 non-null    object\n",
      " 2   cosim      561 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 13.3+ KB\n"
     ]
    }
   ],
   "source": [
    "output.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e775c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "output.to_csv(\"자기자신을포함한top10유사웹툰.csv\",encoding=\"euc-kr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3e26fb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ba5be5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\smhrd\\\\Desktop\\\\thirdproject'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca46843",
   "metadata": {},
   "source": [
    "### 여기부턴 장르 + 태그를 합친 시놉시스 유사도 함 찍어보는기\n",
    "data = 웹툰 전체 정보\n",
    "data2 = 시놉시시스 요약본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bc5c685f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다합친 리스트하나뽑고\n",
    "test_list = data2['synopsis']+\" \"+data['genre']+data['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "fddbfef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"jhgan/ko-sbert-sts\")\n",
    "embeddings = model.encode(test_list, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(test, convert_to_tensor=True)\n",
    "# embeddings2 = model.encode(data['synopsis'],\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "cosine_scores = cosine_scores.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "73dc6486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "입력 시놉시스 : 무너진 교권을 지키기 위해 교권보호국의 참교육이 시작된다! 액션블루스트링, 사이다, 학원물, 다크히어로, 먼치킨, 액션\n",
      "\n",
      " 젤 비슷한 시놉시스:\n",
      "사신소년 ::  수명을 대가로 죽은 자의 능력을 빌리는 소년! 화려한 액션부터 스포츠, 두뇌 플레이까지 한계는 없다! 액션이능력배틀물, 소년왕도물, 먼치킨, 액션 tensor(0.6429)\n",
      "\n",
      "\n",
      "약한영웅 ::  선천적으로 약한 고등학생 소년이 상대보다 몇 수 앞을 예측하는 심리전과 지형지물을 이용하고, 도구로 살벌하게 끝장내는 파이터로 성장한다. 액션드라마&영화 원작웹툰, 힘숨찐, 학원물, 액션 tensor(0.6353)\n",
      "\n",
      "\n",
      "한림체육관 ::  그릇된 강함을 실천하던 주인공이 체육관에 들어와 진정한 강함을 깨닫고 무도인으로 거듭나는 학원 액션물 액션블루스트링, 슈퍼스트링, 학원물, 소년왕도물, 액션 tensor(0.6236)\n",
      "\n",
      "\n",
      "최강전설 강해효 ::  맹수같이 거친 문제아 학생들만 모여있는 고등학교! 그곳에서 변화가 시작되었다. 액션액션 tensor(0.6094)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 샘플코드\n",
    "top_k = 5\n",
    "jj=0\n",
    "#We use np.argpartition, to only partially sort the top_k results\n",
    "top_results = np.argpartition(-cosine_scores[jj], range(top_k))[0:top_k]\n",
    "# 제일 높은 얘의 index 구하는거임. \n",
    "\n",
    "print(\"\\n\\n======================\\n\\n\")\n",
    "print(\"입력 시놉시스 :\", test_list[jj])\n",
    "print(\"\\n 젤 비슷한 시놉시스:\")\n",
    "\n",
    "k = 0\n",
    "for idx in top_results[0:top_k]:\n",
    "    if k ==0 :\n",
    "        k+=1\n",
    "        continue\n",
    "    print(data['title'][int(idx)]+\" :: \",test_list[int(idx)].strip(),  cosine_scores[jj][int(idx)])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "3b658060",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_list = []\n",
    "title_list =[]\n",
    "temp_list = []\n",
    "temp_list2 = []\n",
    "top10 = 10\n",
    "\n",
    "for i in range(len(data['synopsis'])):\n",
    "    #We use np.argpartition, to only partially sort the op results\n",
    "    top_results = np.argpartition(-cosine_scores[i], range(top10))[0:top10]\n",
    "    # 제일 높은 얘의 index 구하는거임. \n",
    "\n",
    "    \n",
    "    k = 0\n",
    "    temp_list=[]\n",
    "    temp_list2=[]\n",
    "    for idx in top_results[0:top10]:\n",
    "        temp_list.append(data['title'][int(idx)])\n",
    "        temp_list2.append(round(float(cosine_scores[i][int(idx)]),2))\n",
    "    title_list.append(temp_list)\n",
    "    similarity_list.append(temp_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "02b85f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2=pd.DataFrame()\n",
    "output2['title'] =data['title']\n",
    "output2['simtitles']=title_list\n",
    "output2['cosim']=similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "a3a0bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.to_csv(\"top10장르태그포함tostring.csv\",encoding=\"euc-kr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "8cab63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(output2['simtitles'])):\n",
    "    test_list=[]\n",
    "    output2['simtitles'][i] = ','.join(output2['simtitles'][i])\n",
    "    test_list = list(map(str, output2['cosim'][i] ))\n",
    "    test_list = ','.join(test_list)\n",
    "    output2['cosim'][i] = test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "d73e91d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0,0.64,0.64,0.62,0.61,0.61,0.6,0.59,0.59,0.58\n",
       "1        1.0,0.5,0.4,0.39,0.37,0.36,0.36,0.35,0.35,0.34\n",
       "2       1.0,0.69,0.63,0.61,0.6,0.59,0.58,0.58,0.56,0.56\n",
       "3      1.0,0.75,0.74,0.67,0.65,0.65,0.65,0.65,0.64,0.62\n",
       "4      1.0,0.47,0.45,0.45,0.44,0.44,0.43,0.42,0.42,0.41\n",
       "                             ...                       \n",
       "556     1.0,0.56,0.53,0.52,0.52,0.52,0.51,0.51,0.51,0.5\n",
       "557    1.0,0.72,0.68,0.68,0.68,0.67,0.67,0.67,0.67,0.67\n",
       "558    1.0,0.59,0.58,0.57,0.57,0.57,0.57,0.56,0.56,0.55\n",
       "559    1.0,0.66,0.64,0.64,0.59,0.59,0.58,0.58,0.58,0.57\n",
       "560    1.0,0.76,0.74,0.73,0.73,0.72,0.71,0.71,0.71,0.71\n",
       "Name: cosim, Length: 561, dtype: object"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2['cosim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "376bb2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 561 entries, 0 to 560\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   title      561 non-null    object\n",
      " 1   simtitles  561 non-null    object\n",
      " 2   cosim      561 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 13.3+ KB\n"
     ]
    }
   ],
   "source": [
    "output2.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
