{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b13fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "import torch.utils.data as data\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as v_utils\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox, TextArea\n",
    "from matplotlib.cbook import get_sample_data\n",
    "from PIL import ImageFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b779edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "image_size = 256\n",
    "PATH = \"D:testWebtoonsData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f447ab3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1초': 0, '1학년 9반': 1}\n",
      "{0: '1초', 1: '1학년 9반'}\n"
     ]
    }
   ],
   "source": [
    "data = datasets.ImageFolder(PATH,transform= transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Grayscale(num_output_channels=3)\n",
    "        ]))\n",
    "\n",
    "print(data.class_to_idx)\n",
    "\n",
    "class_to_idx = data.class_to_idx\n",
    "idx_to_class = {}\n",
    "for key, value in enumerate(class_to_idx):\n",
    "    idx_to_class[key] = value\n",
    "    \n",
    "print(idx_to_class)\n",
    "\n",
    "img_list = []\n",
    "for i in data.imgs:\n",
    "    img_list.append(i[0])\n",
    "\n",
    "#img_list2 = []\n",
    "\n",
    "#for img in os.listdir('/content/drive/My Drive/dataset/thumnail'):\n",
    "#    img_list2.append(os.path.join('/content/drive/My Drive/dataset/thumnail',img))\n",
    "#img_list2.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94f9078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smhrd\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\smhrd\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\smhrd/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bcd2ed70774e7f8244f88f13f106f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Resnet,self).__init__()\n",
    "        self.layer0 = nn.Sequential(*list(resnet.children())[0:1])\n",
    "        self.layer1 = nn.Sequential(*list(resnet.children())[1:4])\n",
    "        self.layer2 = nn.Sequential(*list(resnet.children())[4:5])\n",
    "        self.layer3 = nn.Sequential(*list(resnet.children())[5:6])\n",
    "        #self.layer4 = nn.Sequential(*list(resnet.children())[6:7])\n",
    "        #self.layer5 = nn.Sequential(*list(resnet.children())[7:8])\n",
    "\n",
    "    def forward(self,x):\n",
    "        out_0 = self.layer0(x)\n",
    "        out_1 = self.layer1(out_0)\n",
    "        out_2 = self.layer2(out_1)\n",
    "        out_3 = self.layer3(out_2)\n",
    "        #out_4 = self.layer4(out_3)\n",
    "        #out_5 = self.layer5(out_4)\n",
    "\n",
    "        return out_0, out_1, out_2, out_3, # out_4, out_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e025329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b,c,h,w = input.size()\n",
    "        F = input.view(b, c, h*w)\n",
    "        G = torch.bmm(F, F.transpose(1,2)) \n",
    "        return G\n",
    "\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431d22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = Resnet()\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee6b677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 images style feature extracted..[3.0%]\n",
      "100 images style feature extracted..[6.0%]\n",
      "150 images style feature extracted..[10.0%]\n",
      "200 images style feature extracted..[13.0%]\n",
      "250 images style feature extracted..[16.0%]\n",
      "300 images style feature extracted..[19.0%]\n",
      "350 images style feature extracted..[23.0%]\n",
      "400 images style feature extracted..[26.0%]\n",
      "450 images style feature extracted..[28.999999999999996%]\n",
      "500 images style feature extracted..[32.0%]\n",
      "550 images style feature extracted..[36.0%]\n",
      "600 images style feature extracted..[39.0%]\n",
      "650 images style feature extracted..[42.0%]\n",
      "700 images style feature extracted..[45.0%]\n",
      "750 images style feature extracted..[49.0%]\n",
      "800 images style feature extracted..[52.0%]\n",
      "850 images style feature extracted..[55.00000000000001%]\n",
      "900 images style feature extracted..[57.99999999999999%]\n",
      "950 images style feature extracted..[62.0%]\n",
      "1000 images style feature extracted..[65.0%]\n",
      "1050 images style feature extracted..[68.0%]\n",
      "1100 images style feature extracted..[71.0%]\n",
      "1150 images style feature extracted..[74.0%]\n",
      "1200 images style feature extracted..[78.0%]\n",
      "1250 images style feature extracted..[81.0%]\n",
      "1300 images style feature extracted..[84.0%]\n",
      "1350 images style feature extracted..[87.0%]\n",
      "1400 images style feature extracted..[91.0%]\n",
      "1450 images style feature extracted..[94.0%]\n",
      "1500 images style feature extracted..[97.0%]\n",
      "Image style feature extraction done.\n"
     ]
    }
   ],
   "source": [
    "total_arr = []\n",
    "label_arr = []\n",
    "\n",
    "for idx,(image,label) in enumerate(data):\n",
    "    i = image\n",
    "    i = i.view(-1,i.size()[0],i.size()[1],i.size()[2])\n",
    "\n",
    "    style_target = list(GramMatrix()(i) for i in resnet(i))\n",
    "\n",
    "    arr = torch.cat([style_target[0].view(-1),style_target[1].view(-1),style_target[2].view(-1),style_target[3].view(-1)],0)\n",
    "    gram = arr.data.numpy().reshape(1,-1)\n",
    "\n",
    "    total_arr.append(gram.reshape(-1))\n",
    "    label_arr.append(label)\n",
    "\n",
    "    if idx % 50 == 0 and idx != 0:\n",
    "        print(f'{idx} images style feature extracted..[{round(idx / len(data), 2) * 100}%]')\n",
    "print('Image style feature extraction done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09af60ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smhrd\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:790: FutureWarning: The default learning rate in TSNE will change from 200.0 to 'auto' in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 301 nearest neighbors...\n",
      "[t-SNE] Indexed 1544 samples in 0.399s...\n",
      "[t-SNE] Computed neighbors for 1544 samples in 17.866s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1544\n",
      "[t-SNE] Computed conditional probabilities for sample 1544 / 1544\n",
      "[t-SNE] Mean sigma: 19858.847902\n",
      "[t-SNE] Computed conditional probabilities in 0.472s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smhrd\\anaconda3\\lib\\site-packages\\sklearn\\manifold\\_t_sne.py:982: FutureWarning: The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Iteration 50: error = 46.1192474, gradient norm = 0.0006640 (50 iterations in 0.426s)\n",
      "[t-SNE] Iteration 100: error = 45.3067818, gradient norm = 0.0003552 (50 iterations in 0.406s)\n",
      "[t-SNE] Iteration 150: error = 44.7790794, gradient norm = 0.0002431 (50 iterations in 0.369s)\n",
      "[t-SNE] Iteration 200: error = 44.3901978, gradient norm = 0.0001836 (50 iterations in 0.371s)\n",
      "[t-SNE] Iteration 250: error = 44.0851669, gradient norm = 0.0001470 (50 iterations in 0.371s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 44.085167\n",
      "[t-SNE] Iteration 300: error = 1.1828787, gradient norm = 0.0001505 (50 iterations in 0.367s)\n",
      "[t-SNE] Iteration 350: error = 1.1679806, gradient norm = 0.0001336 (50 iterations in 0.368s)\n",
      "[t-SNE] Iteration 400: error = 1.1488595, gradient norm = 0.0001148 (50 iterations in 0.372s)\n",
      "[t-SNE] Iteration 450: error = 1.1290512, gradient norm = 0.0000985 (50 iterations in 0.369s)\n",
      "[t-SNE] Iteration 500: error = 1.1101335, gradient norm = 0.0000852 (50 iterations in 0.366s)\n",
      "[t-SNE] Iteration 550: error = 1.0925386, gradient norm = 0.0000747 (50 iterations in 0.372s)\n",
      "[t-SNE] Iteration 600: error = 1.0763810, gradient norm = 0.0000664 (50 iterations in 0.371s)\n",
      "[t-SNE] Iteration 650: error = 1.0614954, gradient norm = 0.0000596 (50 iterations in 0.376s)\n",
      "[t-SNE] Iteration 700: error = 1.0477605, gradient norm = 0.0000541 (50 iterations in 0.373s)\n",
      "[t-SNE] Iteration 750: error = 1.0350389, gradient norm = 0.0000494 (50 iterations in 0.374s)\n",
      "[t-SNE] Iteration 800: error = 1.0231884, gradient norm = 0.0000455 (50 iterations in 0.379s)\n",
      "[t-SNE] Iteration 850: error = 1.0121176, gradient norm = 0.0000422 (50 iterations in 0.375s)\n",
      "[t-SNE] Iteration 900: error = 1.0017421, gradient norm = 0.0000393 (50 iterations in 0.367s)\n",
      "[t-SNE] Iteration 950: error = 0.9919721, gradient norm = 0.0000368 (50 iterations in 0.373s)\n",
      "[t-SNE] Iteration 1000: error = 0.9827392, gradient norm = 0.0000346 (50 iterations in 0.364s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 0.982739\n"
     ]
    }
   ],
   "source": [
    "model = TSNE(n_components=2, init='pca',random_state=0, verbose=3, perplexity=100)\n",
    "result = model.fit_transform(total_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad8e1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[:2291][0].mean(), result[:2291][1].mean())\n",
    "print(label_arr[2291])\n",
    "print(result[2292:][0].mean(),result[2292:][1].mean())\n",
    "print(label_arr[2292])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f2f5eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 840, 1544]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_7284\\854956500.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_img_vec = result_img_vec.append({'webtoon_title':[idx_to_class[label_arr[divide_idx[i]]]],\n",
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_7284\\854956500.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result_img_vec = result_img_vec.append({'webtoon_title':[idx_to_class[label_arr[divide_idx[i]]]],\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>webtoon_title</th>\n",
       "      <th>vec_Xmean</th>\n",
       "      <th>vec_Ymean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1초]</td>\n",
       "      <td>[30986.615]</td>\n",
       "      <td>[34373.504]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1학년 9반]</td>\n",
       "      <td>[6509.3223]</td>\n",
       "      <td>[40625.094]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  webtoon_title    vec_Xmean    vec_Ymean\n",
       "0          [1초]  [30986.615]  [34373.504]\n",
       "1      [1학년 9반]  [6509.3223]  [40625.094]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_img_vec = pd.DataFrame({'webtoon_title':[], 'vec_Xmean':[], 'vec_Ymean':[]})\n",
    "divide_idx = []\n",
    "\n",
    "for i in range(len(result)):\n",
    "    if(i == 0):\n",
    "        divide_idx.append(i)\n",
    "    if(i == (len(result) - 1)):\n",
    "        divide_idx.append(len(result))\n",
    "    if((i != 0) & (label_arr[i-1] != label_arr[i])):\n",
    "        divide_idx.append(i)\n",
    "\n",
    "print(divide_idx)\n",
    "\n",
    "for i in range(len(divide_idx) - 1):\n",
    "    result_img_vec = result_img_vec.append({'webtoon_title':[idx_to_class[label_arr[divide_idx[i]]]],\n",
    "                                            'vec_Xmean':[result[divide_idx[i]:divide_idx[i + 1]][0].mean()],\n",
    "                                            'vec_Ymean':[result[divide_idx[i]:divide_idx[i + 1]][1].mean()]},\n",
    "                                            ignore_index=True)\n",
    "\n",
    "result_img_vec.to_csv('D:result_webtoon_vec.csv', encoding='UTF-8')\n",
    "result_img_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc3fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imscatter(x, y, image, ax=None, zoom=1, show_by_thumnail=False, title='webtoon'):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    try:\n",
    "        image = plt.imread(image)\n",
    "    except TypeError:\n",
    "        # Likely already an array...\n",
    "        pass\n",
    "    im = OffsetImage(image, zoom=zoom)\n",
    "\n",
    "    # Convert inputs to arrays with at least one dimension.\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    \n",
    "    artists = []\n",
    "    for x0, y0 in zip(x, y):\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        \n",
    "\n",
    "        if show_by_thumnail:\n",
    "            offsetbox = TextArea(title, minimumdescent=False)\n",
    "            ac = AnnotationBbox(offsetbox, (x0, y0),\n",
    "                        xybox=(20, -40),\n",
    "                        xycoords='data',\n",
    "                        boxcoords=\"offset points\")\n",
    "            artists.append(ax.add_artist(ac))\n",
    "        artists.append(ax.add_artist(ab))\n",
    "\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e60d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for i in range(len(result)):\n",
    "    img_path = img_list[i]\n",
    "    imscatter(result[i,0],result[i,1], image=img_path, zoom=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_list = []\n",
    "scatter_x = result[:, 0]\n",
    "scatter_y = result[:, 1]\n",
    "group = np.array(label_arr)\n",
    "\n",
    "for g in np.unique(group):\n",
    "    i = np.where(group==g)\n",
    "    x_avg = np.mean(scatter_x[i])\n",
    "    y_avg = np.mean(scatter_y[i])\n",
    "    avg_list.append((x_avg, y_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7be1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for i in range(len(avg_list)):\n",
    "    img_path = img_list2[i]\n",
    "    imscatter(avg_list[i][0],avg_list[i][1], image=img_path,zoom=0.6, show_by_thumnail=True, title=idx_to_class[i])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "3d897ffbc13b5710e71e9fb36eb41d8dd58a75d8f4bd294893ab0962f9ec9b14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
